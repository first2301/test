{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain_community.vectorstores import ElasticsearchStore\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "MODEL_PATH = os.path.join(project_root, \"ai_models\", \"hyperclova\", \"hyperclova-seed-text-1.5b-q4-k-m.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_system_prompt.txt 로드\n",
    "with open(\"./prompts/system/base_system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    base_system_prompt = f.read()\n",
    "\n",
    "# qa_prompt.txt 로드 \n",
    "\"F:\\chat_test\\prompt\\prompts\\tasks\\prompts\\tasks\\qa_prompt.txt\"\n",
    "with open(\"./prompts/tasks/qa_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"../ai_models/base_models/BGE-m3-ko\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = ElasticsearchStore(\n",
    "    es_url=\"http://localhost:9200\",\n",
    "    index_name=\"documents\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True,\n",
    "    n_ctx=2048,  # 컨텍스트 길이\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=f\"{base_system_prompt}\\n{{context}}\\n\\n{qa_prompt}\"\n",
    "prompt_template = PromptTemplate.from_template(template=template, template_format=\"f-string\")\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,                     # llama-cpp나 OpenAI 등 langchain-compatible LLM\n",
    "    retriever=retriever,         # langchain-compatible retriever\n",
    "    chain_type=\"stuff\",          # \"stuff\", \"map_reduce\", \"refine\" 중 선택\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}  # PromptTemplate을 전달\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"피지컬컬 AI 시장 동향 알려줘\").get(\"result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
