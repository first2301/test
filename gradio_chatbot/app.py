from llama_cpp import Llama
import gradio as gr

# ëª¨ë¸ ë¡œë”©
llm = Llama(
    model_path="../ai_models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=8,
    n_batch=64
)

def chat_with_bot(message, history):
    # ì•„ì´ì½˜ ë° ì´ë¦„ ë¼ë²¨
    user_icon = "ğŸ™‹â€â™‚ï¸"
    ai_icon = "ğŸ¤–"

    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš°ë©° ìœ ìµí•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. \n
                ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì™€ ë§¥ë½ì„ ì •í™•íˆ íŒŒì•…í•˜ê³ , êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”. \n
                ê°„ê²°í•˜ê²Œ í•µì‹¬ì„ ì „ë‹¬í•˜ë©´ì„œë„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì˜ˆì‹œë‚˜ ì¶”ê°€ ì •ë³´ë¥¼ í¬í•¨í•´ ë‹µë³€í•˜ì„¸ìš”. \n
                í•„ìš”í•˜ë‹¤ë©´ ì¹œê·¼í•œ ì–´íˆ¬ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”. \n
                ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n
                ë‹µë³€ì„ ìˆœì„œëŒ€ë¡œ ì•Œë ¤ì£¼ì–´ì•¼ í•˜ëŠ” ê²½ìš°, ë¶ˆë¦¿í¬ì¸íŠ¸ ë˜ëŠ” ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.\n
                """
    chat_history = ""
    for human, ai in history:
        chat_history += f"ì‚¬ìš©ì: {human}\nAI: {ai}\n"
    prompt = f"{system_prompt}{chat_history}ì‚¬ìš©ì: {message}\nAI:"

    output = llm(
        prompt,
        max_tokens=512,
        stop=["ì‚¬ìš©ì:", "AI:"],
        echo=False,
        temperature=0.7,
        top_p=0.9
    )

    bot_response = output["choices"][0]["text"].strip()

    # ë©”ì‹œì§€ HTML í¬ë§·
    user_msg = f"{user_icon} <b>ì‚¬ìš©ì:</b><br>{message}"
    formatted_response = f"{ai_icon} <b>AI:</b><br>{bot_response}"
    
    history.append((user_msg, formatted_response))
    return "", history, history

# Gradio UI êµ¬ì„±
with gr.Blocks() as demo:
    gr.Markdown("## ğŸŒ¸ <b>LLaMA-3.2 Korean Bllossom 3B Chatbot</b>")
    
    chatbot = gr.Chatbot(show_label=False, bubble_full_width=False)
    msg = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="ì…ë ¥")
    clear = gr.Button("ì´ˆê¸°í™”")

    state = gr.State([])

    msg.submit(chat_with_bot, [msg, state], [msg, chatbot, state])
    clear.click(lambda: ([], []), None, [chatbot, state])

demo.launch(share=True)  # í•„ìš”ì‹œ share=True

# from llama_cpp import Llama
# import gradio as gr

# # ëª¨ë¸ ë¡œë”©
# llm = Llama(
#     model_path="../ai_models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
#     n_ctx=2048,
#     n_threads=8,   # ì‚¬ìš© í™˜ê²½ì— ë§ê²Œ ì¡°ì ˆ
#     n_batch=64
# )

# # ì±—ë´‡ ì‘ë‹µ í•¨ìˆ˜
# def chat_with_bot(message, history):
#     system_prompt = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤.\n"
#     chat_history = ""
#     for human, ai in history:
#         chat_history += f"ì‚¬ìš©ì: {human}\nAI: {ai}\n"
#     prompt = f"{system_prompt}{chat_history}ì‚¬ìš©ì: {message}\nAI:"

#     output = llm(
#         prompt,
#         max_tokens=512,
#         stop=["ì‚¬ìš©ì:", "AI:"],
#         echo=False,
#         temperature=0.7,
#         top_p=0.9
#     )

#     bot_response = output["choices"][0]["text"].strip()
#     history.append((message, bot_response))
    
#     # âœ… ë°˜ë“œì‹œ 3ê°œë¥¼ ë°˜í™˜í•´ì•¼ í•¨ (Textbox, Chatbot, State)
#     return "", history, history


# # Gradio UI êµ¬ì„±
# with gr.Blocks() as demo:
#     gr.Markdown("## ğŸŒ¸ LLaMA-3.2 Korean Bllossom 3B Chatbot")
    
#     chatbot = gr.Chatbot()
#     msg = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="ì…ë ¥")
#     clear = gr.Button("ì´ˆê¸°í™”")

#     state = gr.State([])

#     msg.submit(chat_with_bot, [msg, state], [msg, chatbot, state])
#     clear.click(lambda: ([], []), None, [chatbot, state])

# demo.launch()
