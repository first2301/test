from langchain.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
import gradio as gr

# âœ… System Prompt ì„¤ì •
system_prompt = """
ë‹¹ì‹ ì€ ì§€ì‹ì´ í’ë¶€í•˜ê³ , ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ì´ë„ëŠ” í•œêµ­ì–´ ì „ìš© AI ì±—ë´‡ì…ë‹ˆë‹¤.

- í•­ìƒ **ì´ì „ ëŒ€í™” ë‚´ìš©(history)**ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë§¥ë½ê³¼ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ë§íˆ¬, ë‹¨ì–´ ì„ íƒ, ì§ˆë¬¸ êµ¬ì¡° ë“±ì„ ë¶„ì„í•´ **í‘œë©´ì ì¸ ì§ˆë¬¸ ë’¤ì— ìˆ¨ì€ ëª©ì ì´ë‚˜ ê´€ì‹¬ì‚¬**ê¹Œì§€ ì´í•´í•˜ë ¤ê³  ë…¸ë ¥í•˜ì„¸ìš”.
- ì„¤ëª…í•  ë•ŒëŠ” **êµ¬ì²´ì ì´ê³  ë‹¨ê³„ì ì¸ ë°©ì‹**ìœ¼ë¡œ ë‹µë³€í•˜ë©°, í•„ìš” ì‹œ ë²ˆí˜¸(1., 2., 3.)ë‚˜ ë¶ˆë¦¿í¬ì¸íŠ¸(â€¢)ë¥¼ ì‚¬ìš©í•´ êµ¬ì¡°í™”í•˜ì„¸ìš”.
- ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ë‚˜ì—´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ì™œ ì¤‘ìš”í•œì§€**, **ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ”ì§€** ë“±ì˜ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”.
- **ëª¨ë“  ì‘ë‹µì€ ê¸°ë³¸ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±**í•˜ë˜, ì˜ì–´Â·í•œì ë“± ì™¸êµ­ì–´ëŠ” ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³´ì¡° ì„¤ëª… ìš©ë„ë¡œ ê°„ê²°í•˜ê²Œ ì²¨ë¶€í•˜ì„¸ìš”.
- ë³µì¡í•˜ê±°ë‚˜ ìƒì†Œí•œ ê°œë…ì€ ì¼ìƒì ì¸ ì˜ˆì‹œë‚˜ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ì–´ ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ì¹œê·¼í•œ ë§íˆ¬ë¡œ ë³´ì™„í•˜ì„¸ìš”.
"""


# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = """
{system_prompt}

### ëŒ€í™” ì´ë ¥:
{history}

### ì‚¬ìš©ì: {input}
AI:"""

# 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì—ì„œ system_promptë¥¼ ì§ì ‘ í¬í•¨
prompt = PromptTemplate(
    input_variables=["history", "input"],
    template=f"""{system_prompt}

### ëŒ€í™” ì´ë ¥:
{{history}}

### ì‚¬ìš©ì: {{input}}
AI:"""
)

# âœ… LLaMA ëª¨ë¸ ë¡œë”©
llm = LlamaCpp(
    model_path="../ai_models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
    n_ctx=2048,
    n_batch=64,
    n_threads=8,
    temperature=0.7,
    verbose=True
)

# âœ… LangChain ë©”ëª¨ë¦¬ êµ¬ì„±
memory = ConversationBufferMemory(return_messages=False)

# âœ… LLMChain êµ¬ì„±
conversation = LLMChain(
    llm=llm,
    prompt=prompt,
    memory=memory,
    verbose=True
)

# âœ… Gradio ì—°ê²° í•¨ìˆ˜
def chat_with_memory(message, history):
    response = conversation.predict(input=message)

    user_icon = "ğŸ§‘â€ğŸ’»"
    ai_icon = "ğŸ¤–"

    user_msg = f"{user_icon} <b>ì‚¬ìš©ì:</b><br>{message}"
    ai_msg = f"{ai_icon} <b>AI:</b><br>{response}"

    history.append((user_msg, ai_msg))
    return "", history, history

# âœ… Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ¤– <b>LLaMA + LangChain ì¥ê¸° ê¸°ì–µ ì±—ë´‡</b>")

    chatbot = gr.Chatbot(show_label=False, bubble_full_width=False)
    msg = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="ì…ë ¥")
    clear = gr.Button("ì´ˆê¸°í™”")

    state = gr.State([])

    msg.submit(chat_with_memory, [msg, state], [msg, chatbot, state])
    clear.click(lambda: ([], []), None, [chatbot, state])

demo.launch(share=True)


# from langchain.llms import LlamaCpp
# from langchain.chains import ConversationChain
# from langchain.memory import ConversationBufferMemory
# import gradio as gr

# # LLaMA ëª¨ë¸ ê²½ë¡œ ì„¤ì •
# llm = LlamaCpp(
#     model_path="../ai_models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
#     n_ctx=2048,
#     n_batch=64,
#     n_threads=8,
#     temperature=0.7,
#     verbose=True
# )

# # LangChain Memory ê°ì²´ (ëŒ€í™” ì €ì¥)
# memory = ConversationBufferMemory()

# # ëŒ€í™” ì²´ì¸ êµ¬ì„±
# conversation = ConversationChain(
#     llm=llm,
#     memory=memory,
#     verbose=True
# )

# # Gradio ì—°ê²°
# def chat_with_memory(message, history):
#     response = conversation.predict(input=message)
#     history.append((message, response))
#     return "", history, history

# with gr.Blocks() as demo:
#     gr.Markdown("## ğŸ¤– LLaMA + LangChain ì¥ê¸° ê¸°ì–µ ì±—ë´‡")

#     chatbot = gr.Chatbot(show_label=False, bubble_full_width=False)
#     msg = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="ì…ë ¥")
#     clear = gr.Button("ì´ˆê¸°í™”")

#     state = gr.State([])

#     msg.submit(chat_with_memory, [msg, state], [msg, chatbot, state])
#     clear.click(lambda: ([], []), None, [chatbot, state])

# demo.launch(share=True)
