from langchain.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
import gradio as gr

# âœ… System Prompt ì„¤ì •
system_prompt = """
ë‹¹ì‹ ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ë©°, ì •ë³´ ì „ë‹¬ê³¼ ë§¥ë½ ì´í•´ì— ì§‘ì¤‘í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.

â— ì£¼ì˜ì‚¬í•­:
- **ì ˆëŒ€ ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ AIê°€ ë¨¼ì € ì§ˆë¬¸í•˜ê±°ë‚˜ ëŒ€í™”ë¥¼ ì´ëŒì–´ê°€ì§€ ë§ˆì„¸ìš”.**
- **ì‚¬ìš©ìê°€ ë§í•˜ê¸° ì „ì—ëŠ” AIê°€ ìì˜ì ìœ¼ë¡œ ë°œí™”í•˜ê±°ë‚˜ ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” í–‰ë™ì„ ê¸ˆì§€í•©ë‹ˆë‹¤.**
- ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ì§€ ì•Šì•˜ë”ë¼ë„, AIê°€ ë¨¼ì € ì¶”ì¸¡í•˜ì—¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ê±°ë‚˜ ê·¸ì— ëŒ€í•´ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

ğŸ¤– ì—­í•  ë° ì‘ë‹µ ë°©ì‹:
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê³¼ê±° ëŒ€í™”(history)ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ë§¥ë½ì„ ì´í•´í•˜ê³ ** ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆë¶„ëª…í•œ ê²½ìš°ì—ë§Œ **ì§§ê³  ëª…í™•í•œ í™•ì¸ ì§ˆë¬¸**ìœ¼ë¡œ ë³´ì™„ ì •ë³´ë¥¼ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„¤ëª…ì€ êµ¬ì²´ì ì´ê³  ë‹¨ê³„ì ìœ¼ë¡œ êµ¬ì„±í•˜ë©°, í•„ìš”í•  ê²½ìš° ë²ˆí˜¸(1., 2., 3.) ë˜ëŠ” ë¶ˆë¦¿í¬ì¸íŠ¸(â€¢) í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•˜ê²Œ ê¸¸ê±°ë‚˜ ë°˜ë³µì ì¸ ì„¤ëª…ì€ í”¼í•˜ê³ , í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ **ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´**ë¡œ ì‘ì„±í•´ì•¼ í•˜ë©°, ì™¸êµ­ì–´(ì˜ì–´, í•œì ë“±)ëŠ” **ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³´ì¡°ë¡œ ê°„ë‹¨íˆ ì²¨ë¶€**í•˜ì„¸ìš”.
- ë³µì¡í•˜ê±°ë‚˜ ìƒì†Œí•œ ê°œë…ì€ ì¼ìƒì ì¸ ì˜ˆì‹œë‚˜ ì‰¬ìš´ ì–¸ì–´ë¡œ í’€ì–´ ì„¤ëª…í•˜ì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì™¸êµ­ì–´(ì˜ì–´, í•œì ë“±)ëŠ” ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ê´„í˜¸ ì† ì§§ì€ ë³´ì¡° ì„¤ëª…ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- í•œê¸€ ì™¸ ë‹¨ì–´ê°€ ìë™ìœ¼ë¡œ ì‚½ì…ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ë©°, íŠ¹íˆ ëª…ì‚¬Â·í˜•ìš©ì‚¬ ë“±ì€ í•œêµ­ì–´ í‘œí˜„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë¶ˆë¶„ëª…í•œ ê²½ìš°, ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ í†µí•´ ì§ˆë¬¸ì„ ëª…í™•íˆ í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.
"""



# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = """
{system_prompt}

### ëŒ€í™” ì´ë ¥:
{history}

### ì‚¬ìš©ì: {input}
AI:"""

# 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì—ì„œ system_promptë¥¼ ì§ì ‘ í¬í•¨
prompt = PromptTemplate(
    input_variables=["history", "input"],
    template=f"""{system_prompt}

### ëŒ€í™” ì´ë ¥:
{{history}}

### ì‚¬ìš©ì: {{input}}
AI:"""
)

# âœ… LLaMA ëª¨ë¸ ë¡œë”©
llm = LlamaCpp(
    model_path="../ai_models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
    # model_path="../ai_models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q8_0.gguf",
    # model_path="../ai_models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
    n_ctx=2048,
    n_batch=64,
    n_threads=8,
    temperature=0.7,
    stop=["ì‚¬ìš©ì:", "ì‚¬ìš©ì: ", "User:", "User: ","AI:", "AI: ", "Assistant:", "Assistant: "],
    # top_p=0.95,
    verbose=True
)

# âœ… LangChain ë©”ëª¨ë¦¬ êµ¬ì„±
memory = ConversationBufferMemory(return_messages=False)

# âœ… LLMChain êµ¬ì„±
conversation = LLMChain(
    llm=llm,
    prompt=prompt,
    memory=memory,
    verbose=True
)

# âœ… Gradio ì—°ê²° í•¨ìˆ˜
def chat_with_memory(message, history):
    response = conversation.predict(input=message)

    user_icon = "ğŸ§‘â€ğŸ’»"
    ai_icon = "ğŸ¤–"

    user_msg = f"{user_icon} <b>ì‚¬ìš©ì:</b><br>{message}"
    ai_msg = f"{ai_icon} <b>AI:</b><br>{response}"

    history.append((user_msg, ai_msg))
    return "", history, history

# âœ… Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ¤– <b>PCN R&D Chatbot</b>")

    chatbot = gr.Chatbot(show_label=False, bubble_full_width=False)
    msg = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="ì…ë ¥")
    clear = gr.Button("ì´ˆê¸°í™”")

    state = gr.State([])

    msg.submit(chat_with_memory, [msg, state], [msg, chatbot, state])
    clear.click(lambda: ([], []), None, [chatbot, state])

demo.launch(share=True)


# from langchain.llms import LlamaCpp
# from langchain.chains import ConversationChain
# from langchain.memory import ConversationBufferMemory
# import gradio as gr

# # LLaMA ëª¨ë¸ ê²½ë¡œ ì„¤ì •
# llm = LlamaCpp(
#     model_path="../ai_models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
#     n_ctx=2048,
#     n_batch=64,
#     n_threads=8,
#     temperature=0.7,
#     verbose=True
# )

# # LangChain Memory ê°ì²´ (ëŒ€í™” ì €ì¥)
# memory = ConversationBufferMemory()

# # ëŒ€í™” ì²´ì¸ êµ¬ì„±
# conversation = ConversationChain(
#     llm=llm,
#     memory=memory,
#     verbose=True
# )

# # Gradio ì—°ê²°
# def chat_with_memory(message, history):
#     response = conversation.predict(input=message)
#     history.append((message, response))
#     return "", history, history

# with gr.Blocks() as demo:
#     gr.Markdown("## ğŸ¤– LLaMA + LangChain ì¥ê¸° ê¸°ì–µ ì±—ë´‡")

#     chatbot = gr.Chatbot(show_label=False, bubble_full_width=False)
#     msg = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="ì…ë ¥")
#     clear = gr.Button("ì´ˆê¸°í™”")

#     state = gr.State([])

#     msg.submit(chat_with_memory, [msg, state], [msg, chatbot, state])
#     clear.click(lambda: ([], []), None, [chatbot, state])

# demo.launch(share=True)
