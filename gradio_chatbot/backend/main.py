from fastapi import FastAPI, Request
from pydantic import BaseModel
from langchain.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from fastapi.middleware.cors import CORSMiddleware

# âœ… ì‚¬ìš©ì ì…ë ¥ í˜•ì‹ ì •ì˜
class ChatRequest(BaseModel):
    message: str

# âœ… ì‘ë‹µ í˜•ì‹ ì •ì˜
class ChatResponse(BaseModel):
    response: str

# âœ… ì±—ë´‡ í´ë˜ìŠ¤ ì •ì˜
class KoreanChatbot:
    def __init__(self):
        self._init_prompt()
        self._init_llm()
        self._init_chain()

    def _init_prompt(self):
        self.system_prompt = """
ë‹¹ì‹ ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ë©°, ì •ë³´ ì „ë‹¬ê³¼ ë§¥ë½ ì´í•´ì— ì§‘ì¤‘í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.

â— ì£¼ì˜ì‚¬í•­:
- **ì ˆëŒ€ ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ AIê°€ ë¨¼ì € ì§ˆë¬¸í•˜ê±°ë‚˜ ëŒ€í™”ë¥¼ ì´ëŒì–´ê°€ì§€ ë§ˆì„¸ìš”.**
- **ì‚¬ìš©ìê°€ ë§í•˜ê¸° ì „ì—ëŠ” AIê°€ ìì˜ì ìœ¼ë¡œ ë°œí™”í•˜ê±°ë‚˜ ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” í–‰ë™ì„ ê¸ˆì§€í•©ë‹ˆë‹¤.**
- ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ì§€ ì•Šì•˜ë”ë¼ë„, AIê°€ ë¨¼ì € ì¶”ì¸¡í•˜ì—¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ê±°ë‚˜ ê·¸ì— ëŒ€í•´ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

ğŸ¤– ì—­í•  ë° ì‘ë‹µ ë°©ì‹:
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê³¼ê±° ëŒ€í™”(history)ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ë§¥ë½ì„ ì´í•´í•˜ê³ ** ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆë¶„ëª…í•œ ê²½ìš°ì—ë§Œ **ì§§ê³  ëª…í™•í•œ í™•ì¸ ì§ˆë¬¸**ìœ¼ë¡œ ë³´ì™„ ì •ë³´ë¥¼ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„¤ëª…ì€ êµ¬ì²´ì ì´ê³  ë‹¨ê³„ì ìœ¼ë¡œ êµ¬ì„±í•˜ë©°, í•„ìš”í•  ê²½ìš° ë²ˆí˜¸(1., 2., 3.) ë˜ëŠ” ë¶ˆë¦¿í¬ì¸íŠ¸(â€¢) í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•˜ê²Œ ê¸¸ê±°ë‚˜ ë°˜ë³µì ì¸ ì„¤ëª…ì€ í”¼í•˜ê³ , í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ **ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´**ë¡œ ì‘ì„±í•´ì•¼ í•˜ë©°, ì™¸êµ­ì–´(ì˜ì–´, í•œì ë“±)ëŠ” **ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³´ì¡°ë¡œ ê°„ë‹¨íˆ ì²¨ë¶€**í•˜ì„¸ìš”.
- ë³µì¡í•˜ê±°ë‚˜ ìƒì†Œí•œ ê°œë…ì€ ì¼ìƒì ì¸ ì˜ˆì‹œë‚˜ ì‰¬ìš´ ì–¸ì–´ë¡œ í’€ì–´ ì„¤ëª…í•˜ì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì™¸êµ­ì–´(ì˜ì–´, í•œì ë“±)ëŠ” ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ê´„í˜¸ ì† ì§§ì€ ë³´ì¡° ì„¤ëª…ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- í•œê¸€ ì™¸ ë‹¨ì–´ê°€ ìë™ìœ¼ë¡œ ì‚½ì…ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ë©°, íŠ¹íˆ ëª…ì‚¬Â·í˜•ìš©ì‚¬ ë“±ì€ í•œêµ­ì–´ í‘œí˜„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë¶ˆë¶„ëª…í•œ ê²½ìš°, ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ í†µí•´ ì§ˆë¬¸ì„ ëª…í™•íˆ í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.
"""
        self.template = f"""{self.system_prompt}

### ëŒ€í™” ì´ë ¥:
{{history}}

### ì‚¬ìš©ì: {{input}}
AI:"""
        self.prompt = PromptTemplate(
            input_variables=["history", "input"],
            template=self.template
        )

    def _init_llm(self):
        self.llm = LlamaCpp(
            # model_path="../ai_models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q8_0.gguf",
            model_path="../../ai_models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
            n_ctx=2048,
            n_batch=64,
            n_threads=8,
            temperature=0.7,
            stop=["ì‚¬ìš©ì:", "User:", "AI:", "Assistant:"],
            verbose=True
        )

    def _init_chain(self):
        self.memory = ConversationBufferMemory(return_messages=False)
        self.chain = LLMChain(
            llm=self.llm,
            prompt=self.prompt,
            memory=self.memory,
            verbose=True
        )

    def chat(self, message: str) -> str:
        return self.chain.predict(input=message)


# âœ… FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# âœ… CORS ì„¤ì • (í•„ìš”ì‹œ í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„  ë„ë©”ì¸ ì œí•œ ê¶Œì¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
bot = KoreanChatbot()

# âœ… POST /chat ì—”ë“œí¬ì¸íŠ¸
@app.post("/chat", response_model=ChatResponse)
def chat_endpoint(req: ChatRequest):
    response_text = bot.chat(req.message)
    return ChatResponse(response=response_text)
