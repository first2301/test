{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index\n",
    "- https://python.langchain.com/docs/tutorials/rag/#next-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-80GB\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print((torch.cuda._get_nvml_device_index(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = load_dataset(\"../data/wikipedia-korean-20240501-1million-qna/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"나혜석이 1930년대 신문삽화 '섣달대목'에서 명절이 여성들에게 고단한 날임을 지적한 이유는 무엇인가요?\",\n",
       " 'answer': '나혜석은 명절이 여성들에게 고단한 날임을 지적한 것은 그들이 일상과 가사노동에 치여 눈코 뜰 새 없이 분주한 섣달의 풍경을 담고 있으며, 계속해서 신문과 잡지에 발표하는 만평형식의 목판화에도 신, 구 여성의 고달픈 일상에 대한 연민을 나타냈다고 생각했다. 또한 그는 명절이 여자들에게만 일을 시키는 고통스러운 날이라고 지적했다.',\n",
       " 'context': \"성들의 일상과 가사노동을 중심으로 눈코 뜰 새 없이 분주한 섣달의 풍경을 담고 있으며, 계속해서 신문과 잡지에 발표하는 만평형식의 목판화에도 신, 구 여성의 고달픈 일상에 대한 연민 을 나타냈다.\\n또한 그는 명절이 여자들에게만 일을 시키는 고통스러운 날이라고 지적했다. 나혜석이 1930년대 신문삽화 '섣달대목'으로 일찌감치 명절이 여성들에게 고단한 날임을 고발하였다. 그가 명절의 고단함을 지적한 것은 후일 '명절 증후군'이라는 이름으로 사회적 화두가 되기도 했다.\\n=== 사회 개혁론 ===\\n그는 유럽 여행을 마치고 귀국한 후 여행기 ‘구미유기’에서 영국 참정권 운동을 소개하였다. 개화파의 실패 이후 참정권에 거부반응을 보이던 백성들을 향해, 국민이 정치에 참여하는 것은 당연한 것이라며 민주주의와 참정권의 당위성을 역설하였다. 그러나 정치인이나 정부를 양반의 연장으로 보고, 상전처럼 여기던 당시의 백성들은 그의 참정권 주장을 이상하게 여겼으며, 개화당의 아류, 여자 개화당 정도로 취급하며 무시하였다.\\n영국의 참정권 운동을 소개하면서 참여한 영국여성운동가의 활약을 알렸다. 인간평등에 기초한 참정권운동뿐만 아니라 노동, 정조, 이혼, 산아제한, 시험결혼 등 여성문제를 소개하였다. 이후 언론과 칼럼, 강연을 통해 노동 문제, 임금 인상, 해고되지 않을 권리, 정당한 노동 등의 문제를 다루었고, 정조 문제, 결혼의 부작용을 줄일 수 있는 동거혼 등에 대해서도 소개하였다.\\n또한 그는 양복, 양장, 서양식 의류를 입으며 양복과 양장이 쉽게 입고, 벗기 편하다는 점을 소개하고 알리기도 했다. 나혜석 등 1920년~30년대 '모던걸'들도 세련된 양장을 입었지만 한국 여성들이 본격적으로 양장을 입기 시작한 것은 50년대 한국전쟁이 끝난 후부터였다.\\n=== 여성 계몽 운동 ===\\n프랑스로 출국하기 직전 (1926년)\\n결혼을 여성을 억압하고 옭죄는 족쇄라고 판단했다. 또한 그는 '이혼의 비극은 여성 해방으로 예방해야 하고 시험결혼이 필요하다'라는 당시로는 파격적인 칼럼을  잡지에 기고하여 장안의 화\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_dataset = load_dataset(\"json\", data_files={\"train\": \"../data/qna_data/train.json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'도메인': '금융/보험',\n",
       " '카테고리': '사고 및 보상 문의',\n",
       " '대화셋일련번호': 'A4075',\n",
       " '화자': '고객',\n",
       " '문장번호': '1',\n",
       " '고객의도': '비밀번호 오류',\n",
       " '상담사의도': '',\n",
       " 'QA': 'Q',\n",
       " '고객질문(요청)': '인터넷뱅킹 로그인이 안돼요?',\n",
       " '상담사질문(요청)': '',\n",
       " '고객답변': '',\n",
       " '상담사답변': '',\n",
       " '개체명 ': '인터넷뱅킹, 로그인',\n",
       " '용어사전': '인터넷뱅킹/ 금융서비스',\n",
       " '지식베이스': '로그인,금융서비스'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset[\"train\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def preprocess_for_rag(raw_data: list) -> list:\n",
    "    \"\"\"\n",
    "    raw_data: [{'도메인': ..., '카테고리': ..., '고객질문(요청)': ..., '상담사답변': ..., '개체명 ': ..., '지식베이스': ...}, ...]\n",
    "    반환: Document(page_content, metadata) 리스트\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "\n",
    "    for item in raw_data:\n",
    "        # 기본 context 구성\n",
    "        context_parts = []\n",
    "        if item.get(\"고객질문(요청)\"):\n",
    "            context_parts.append(item[\"고객질문(요청)\"])\n",
    "        if item.get(\"상담사답변\"):  # 상담사 답변도 있을 경우 context에 추가\n",
    "            context_parts.append(item[\"상담사답변\"])\n",
    "\n",
    "        # context 하나로 합치기\n",
    "        context = \"\\n\".join(context_parts).strip()\n",
    "\n",
    "        # metadata 구성\n",
    "        metadata = {\n",
    "            \"domain\": item.get(\"도메인\", \"\"),\n",
    "            \"category\": item.get(\"카테고리\", \"\"),\n",
    "            \"customer_intent\": item.get(\"고객의도\", \"\"),\n",
    "            \"agent_intent\": item.get(\"상담사의도\", \"\"),\n",
    "            \"entities\": item.get(\"개체명 \", \"\"),    # 주의: 개체명 뒤에 공백 있음\n",
    "            \"glossary\": item.get(\"용어사전\", \"\"),\n",
    "            \"knowledge_base\": item.get(\"지식베이스\", \"\"),\n",
    "            \"qa_type\": item.get(\"QA\", \"\"),           # 'Q' 또는 'A' 구분\n",
    "            \"dialogue_id\": item.get(\"대화셋일련번호\", \"\"),\n",
    "            \"speaker\": item.get(\"화자\", \"\")\n",
    "        }\n",
    "\n",
    "        # Document 생성\n",
    "        if context:  # context가 비어있지 않으면 추가\n",
    "            doc = Document(page_content=context, metadata=metadata)\n",
    "            processed_documents.append(doc)\n",
    "\n",
    "    return processed_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['도메인', '카테고리', '대화셋일련번호', '화자', '문장번호', '고객의도', '상담사의도', 'QA', '고객질문(요청)', '상담사질문(요청)', '고객답변', '상담사답변', '개체명 ', '용어사전', '지식베이스'],\n",
       "    num_rows: 1782303\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'train' split 선택\n",
    "train_data = qna_dataset['train']\n",
    "\n",
    "# 리스트 of dict 변환\n",
    "try:\n",
    "    raw_data = train_data.to_list()  # 최신 datasets 버전\n",
    "except:\n",
    "    raw_data = train_data.to_pandas().to_dict(orient=\"records\")  # 범용\n",
    "\n",
    "# 이제 전처리\n",
    "docs_for_rag = preprocess_for_rag(raw_data)\n",
    "\n",
    "# # 결과 확인\n",
    "# for doc in docs_for_rag:\n",
    "#     print(doc.page_content)\n",
    "#     print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyPDFLoader, \n",
    "    Docx2txtLoader,\n",
    "    TextLoader,\n",
    "    JSONLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs = JSONLoader(\"../data/wikipedia-korean-20240501-1million-qna/data\", jq_schema='.', text_content=False).load()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_docs = JSONLoader(\"../data/qna_data/train.json\", jq_schema='.', text_content=False).load()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveJsonSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3820349/4211131713.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"../ai_models/base_models/BGE-m3-ko\"\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter = RecursiveJsonSplitter(max_chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_chunks = splitter.split_json(json_data=docs_for_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문장 경계를 잘 인식하는 청커\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,      # 512 tokens 기준 (한글 약 400~500자)\n",
    "    chunk_overlap=80,    # 문맥 이어지게 약 80 tokens 겹치기\n",
    "    separators=[\"\\n\\n\", \"。\", \".\", \"!\", \"?\", \"\\n\"]  # 문장 경계 세밀하게 설정\n",
    ")\n",
    "\n",
    "# 검색된 큰 문단을 문장 기준으로 late chunking\n",
    "chunks = text_splitter.split_documents(docs_for_rag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# docs_for_rag은 이미 Document 리스트 상태\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=docs_for_rag,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# 저장 (로컬 디렉토리 지정)\n",
    "vector_store.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_docs_as_jsonl(documents, output_path):\n",
    "    \"\"\"\n",
    "    documents: List[Document]\n",
    "    output_path: 저장할 파일 경로 (예: 'output.jsonl')\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for doc in documents:\n",
    "            json_obj = {\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            f.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 사용 예시\n",
    "save_docs_as_jsonl(docs_for_rag, \"../data/qna_data/docs_for_rag.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
