{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-80GB\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print((torch.cuda._get_nvml_device_index(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain_community.vectorstores import ElasticsearchStore\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "MODEL_PATH = os.path.join(project_root, \"ai_models\", \"base_models\", \"hyperclova\", \"hyperclova-seed-text-1.5b-q4-k-m.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_system_prompt.txt 로드\n",
    "with open(\"./prompts/system/base_system_prompt_v0.2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    base_system_prompt = f.read()\n",
    "\n",
    "# qa_prompt.txt 로드 \n",
    "with open(\"./prompts/tasks/qa_prompt_v0.2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1244096/1904420697.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"../ai_models/base_models/BGE-m3-ko\",\n",
    "    model_kwargs={'device': 'cuda:0'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_directory = \"./faiss_index_directory\"\n",
    "vectorstore = FAISS.load_local(faiss_index_directory, embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(    \n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\"score_threshold\": 0.5, \"k\": 20}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/AIHUB/PCNRND/home/chatbot/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3607: UserWarning: WARNING! presence_penalty is not default parameter.\n",
      "                presence_penalty was transferred to model_kwargs.\n",
      "                Please confirm that presence_penalty is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/AIHUB/PCNRND/home/chatbot/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3607: UserWarning: WARNING! frequency_penalty is not default parameter.\n",
      "                frequency_penalty was transferred to model_kwargs.\n",
      "                Please confirm that frequency_penalty is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/AIHUB/PCNRND/home/chatbot/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3607: UserWarning: WARNING! device is not default parameter.\n",
      "                device was transferred to model_kwargs.\n",
      "                Please confirm that device is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /AIHUB/PCNRND/home/chatbot/ai_models/base_models/hyperclova/hyperclova-seed-text-1.5b-q4-k-m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hyperclova Seed Text 1.5b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = hyperclova-seed-text\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   5:                            general.license str              = other\n",
      "llama_model_loader: - kv   6:                       general.license.name str              = hyperclovax-seed\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = LICENSE\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 7168\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 100000000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 110592\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = dbrx\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,110592]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,110592]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,110305]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 100275\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 100257\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 100257\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  32:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q4_K:  144 tensors\n",
      "llama_model_loader: - type q6_K:   25 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 956.07 MiB (5.06 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 110523 '<PASSWORD>' is not marked as EOG\n",
      "load: control token: 110522 '<KEY>' is not marked as EOG\n",
      "load: control token: 110521 '<EMAIL>' is not marked as EOG\n",
      "load: control token: 110519 '<pr_diff_hunk_comment_line>' is not marked as EOG\n",
      "load: control token: 110518 '<pr_in_reply_to_comment_id>' is not marked as EOG\n",
      "load: control token: 110517 '<pr_in_reply_to_review_id>' is not marked as EOG\n",
      "load: control token: 110516 '<pr_review_comment>' is not marked as EOG\n",
      "load: control token: 110514 '<pr_review>' is not marked as EOG\n",
      "load: control token: 110513 '<pr_event_id>' is not marked as EOG\n",
      "load: control token: 110512 '<pr_comment>' is not marked as EOG\n",
      "load: control token: 110511 '<pr_diff_hunk>' is not marked as EOG\n",
      "load: control token: 110505 '<pr_status>' is not marked as EOG\n",
      "load: control token: 110503 '<intermediate_to_code>' is not marked as EOG\n",
      "load: control token: 110498 '<jupyter_code>' is not marked as EOG\n",
      "load: control token: 110496 '<jupyter_start>' is not marked as EOG\n",
      "load: control token: 110495 '<issue_closed>' is not marked as EOG\n",
      "load: control token: 110494 '<issue_comment>' is not marked as EOG\n",
      "load: control token: 110493 '<issue_start>' is not marked as EOG\n",
      "load: control token: 110492 '<file_sep>' is not marked as EOG\n",
      "load: control token: 110491 '<repo_name>' is not marked as EOG\n",
      "load: control token: 110504 '<pr>' is not marked as EOG\n",
      "load: control token: 100276 '<|endofprompt|>' is not marked as EOG\n",
      "load: control token: 100271 '<|_unuse_missing_100271|>' is not marked as EOG\n",
      "load: control token: 100269 '<|_unuse_missing_100269|>' is not marked as EOG\n",
      "load: control token: 100268 '<|_unuse_missing_100268|>' is not marked as EOG\n",
      "load: control token: 100267 '<|_unuse_missing_100267|>' is not marked as EOG\n",
      "load: control token: 100263 '<|_unuse_missing_100263|>' is not marked as EOG\n",
      "load: control token: 100262 '<|_unuse_missing_100262|>' is not marked as EOG\n",
      "load: control token: 100259 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 100258 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 100256 '<|_unuse_missing_100256|>' is not marked as EOG\n",
      "load: control token: 110515 '<pr_review_state>' is not marked as EOG\n",
      "load: control token: 100275 '<|endofturn|>' is not marked as EOG\n",
      "load: control token: 100265 '<|_unuse_missing_100265|>' is not marked as EOG\n",
      "load: control token: 110497 '<jupyter_text>' is not marked as EOG\n",
      "load: control token: 110501 '<empty_output>' is not marked as EOG\n",
      "load: control token: 110520 '<NAME>' is not marked as EOG\n",
      "load: control token: 110499 '<jupyter_output>' is not marked as EOG\n",
      "load: control token: 100270 '<|_unuse_missing_100270|>' is not marked as EOG\n",
      "load: control token: 100261 '<|_unuse_missing_100261|>' is not marked as EOG\n",
      "load: control token: 100264 '<|_unuse_missing_100264|>' is not marked as EOG\n",
      "load: control token: 110508 '<pr_file>' is not marked as EOG\n",
      "load: control token: 110502 '<code_to_intermediate>' is not marked as EOG\n",
      "load: control token: 100266 '<|_unuse_missing_100266|>' is not marked as EOG\n",
      "load: control token: 110506 '<pr_is_merged>' is not marked as EOG\n",
      "load: control token: 100272 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 100260 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 110510 '<pr_diff>' is not marked as EOG\n",
      "load: control token: 110509 '<pr_base_code>' is not marked as EOG\n",
      "load: control token: 110500 '<jupyter_script>' is not marked as EOG\n",
      "load: control token: 100274 '<|stop|>' is not marked as EOG\n",
      "load: control token: 110507 '<pr_base>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 54\n",
      "load: token to piece cache size = 0.6841 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 7168\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 100000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 1.59 B\n",
      "print_info: general.name     = Hyperclova Seed Text 1.5b\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 110592\n",
      "print_info: n_merges         = 110305\n",
      "print_info: BOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOS token        = 100275 '<|endofturn|>'\n",
      "print_info: EOT token        = 100273 '<|im_end|>'\n",
      "print_info: UNK token        = 100257 '<|endoftext|>'\n",
      "print_info: PAD token        = 100257 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
      "print_info: EOG token        = 100257 '<|endoftext|>'\n",
      "print_info: EOG token        = 100273 '<|im_end|>'\n",
      "print_info: EOG token        = 100275 '<|endofturn|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 218 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =   956.07 MiB\n",
      "..................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
      "llama_init_from_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.42 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   220.00 MiB\n",
      "llama_init_from_model: graph nodes  = 774\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.file_type': '15', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '7168', 'general.license': 'other', 'llama.attention.value_length': '128', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'general.license.name': 'hyperclovax-seed', 'general.size_label': '1.5B', 'general.type': 'model', 'llama.context_length': '131072', 'general.name': 'Hyperclova Seed Text 1.5b', 'tokenizer.ggml.bos_token_id': '100257', 'general.basename': 'hyperclova-seed-text', 'tokenizer.ggml.padding_token_id': '100257', 'llama.attention.head_count_kv': '8', 'general.architecture': 'llama', 'llama.rope.freq_base': '100000000.000000', 'llama.attention.head_count': '16', 'llama.block_count': '24', 'llama.attention.key_length': '128', 'general.license.link': 'LICENSE', 'tokenizer.ggml.pre': 'dbrx', 'llama.vocab_size': '110592', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '100275', 'tokenizer.ggml.unknown_token_id': '100257', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.add_space_prefix': 'false'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|endofturn|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.6,         # 약간 더 창의적인 응답 유도\n",
    "    top_p=0.9,               # 너무 많은 토큰 허용하지 않도록 조정\n",
    "    max_tokens=512,\n",
    "    repeat_penalty=1.3,\n",
    "    stop=[\"\\n###\",\"<<END>>\"],\n",
    "    presence_penalty=0.2,\n",
    "    frequency_penalty=0.3, \n",
    "    top_k=45,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    n_ctx=2048,\n",
    "    n_gpu_layers=-1,\n",
    "    n_batch=512,\n",
    "    device=\"cuda\",\n",
    "    f16_kv=True\n",
    ")\n",
    "\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=MODEL_PATH,\n",
    "#     temperature=0.6,\n",
    "#     max_tokens=512,\n",
    "#     top_p=1,\n",
    "#     callback_manager=callback_manager,\n",
    "#     verbose=True,\n",
    "#     n_ctx=2048,  # 컨텍스트 길이\n",
    "#     n_gpu_layers=-1,  # 모든 레이어를 GPU에서 실행 (-1은 전체 레이어)\n",
    "#     n_batch=512,  # GPU 배치 크기\n",
    "#     # n_threads=multiprocessing.cpu_count() - 1,\n",
    "#     device=\"cuda\",  # GPU 사용 설정\n",
    "#     f16_kv=True  # GPU 메모리 최적화를 위한 FP16 사용\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template=f\"{base_system_prompt}\\n{{context}}\\n\\n{qa_prompt}\"\n",
    "# prompt_template = PromptTemplate.from_template(template=template) # template_format=\"f-string\"\n",
    "template = f\"\"\"{base_system_prompt}\n",
    "\n",
    "{{context}}\n",
    "\n",
    "{qa_prompt}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,                     # llama-cpp나 OpenAI 등 langchain-compatible LLM\n",
    "    retriever=retriever,         # langchain-compatible retriever\n",
    "    chain_type=\"stuff\",          # \"stuff\", \"map_reduce\", \"refine\" 중 선택\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}  # PromptTemplate을 전달\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초거대 AI 모델이란, 대규모의 데이터를 처리하는데 사용되는 모델을 의미합니다. 초 거대서AI모델은  대량의 데이터가 있는 경우에만 사용 가능하며 사용을 위해서는 반드시 필요한 조건에 부합해야 사용할 수 있습니다.\n",
      "\n",
      "- 규모를 기준으로 삼습니다.\n",
      "- 사용자의 의도를 파악하고 이를 준수하여 일을 처리할 때 다음과 같은 사항을 명세하게 기술한 내용을 주된 사항만 특정하지 않고 상세히 설명하거나 지정합니다. \n",
      "\n",
      "1. **질의 의도**는 초거대 AI 모델은 다음을 수행하기 위한 지침에 따르세요.\n",
      "\n",
      "2. 적어도 3."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   34405.78 ms\n",
      "llama_perf_context_print: prompt eval time =   34405.36 ms /   545 tokens (   63.13 ms per token,    15.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3534.66 ms /   135 runs   (   26.18 ms per token,    38.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   38198.18 ms /   680 tokens\n"
     ]
    }
   ],
   "source": [
    "final_result = rag_chain.invoke(\"초거대 AI 모델이란?\").get(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초거대 AI 모델이란, 대규모의 데이터를 처리하는데 사용되는 모델을 의미합니다. 초 거대서AI모델은  대량의 데이터가 있는 경우에만 사용 가능하며 사용을 위해서는 반드시 필요한 조건에 부합해야 사용할 수 있습니다.\n",
      "\n",
      "- 규모를 기준으로 삼습니다.\n",
      "- 사용자의 의도를 파악하고 이를 준수하여 일을 처리할 때 다음과 같은 사항을 명세하게 기술한 내용을 주된 사항만 특정하지 않고 상세히 설명하거나 지정합니다. \n",
      "\n",
      "1. **질의 의도**는 초거대 AI 모델은 다음을 수행하기 위한 지침에 따르세요.\n",
      "\n",
      "2. 적어도 3.\n"
     ]
    }
   ],
   "source": [
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d5b0a41e-a911-414a-8612-0974f2d7b847', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='.\\x00연구\\x00개요□초거대 AI 모델ㅇ초거대 AI 모델이란, 대규모의 컴퓨팅 인프라를 바탕으로 방대한 데이터를 학습하여 인간처럼 종합적인 인지·판단·추론이 가능해진 ‘큰 규모’의 AI 모델을 의미함(봉강호, 2024)1)\\xad특정 목적에 따라 개별의 데이터를 수집·학습하여 만들어지는 기존의 일반 AI는 학습된 과업(task)에 한하여 수행이 가능한 반면, 초거대 AI는 더욱 복잡하고 광범위한 분야에서 과업을 수행할 수 있음ㅇAI 모델 성능이 ‘모델의 규모(model size)’, ‘학습 데이터(dataset)’ 및 ‘학습 연산(training compute)’의 양에 따라 향상되는 경향성'),\n",
       " Document(id='bfa3c453-5ecc-4106-bdd8-60a4ddf6f5f8', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content=', 주요 모델은 규모 기준 상위 5개로 선정함□연구 자료ㅇ본 연구에서는 美 민간 연구단체인 ‘EPOCH AI’에서 운영 중인 초거대 AI 모델(Large-Scale AI Model) 데이터베이스(DB)에 수록된 정보를 집계하고 분석함1) ‘초거대 언어모델(Large Language Model; LLM)’을 포함하는 개념으로(안성원 외, 2023), 시각, 음성, 생체신호 등의 다양한 유형 모델도 이에 포함됨'),\n",
       " Document(id='9719b420-3a98-416c-a4a0-418b2d0c8b88', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content=': 개; 중복 포함) ㅇ다중 과업을 수행가능한 초거대 AI 모델 수가 2023년을 기점으로 크게 증가하였으며, 다음 해인 2024년에도 증가함(전년 대비 13개 증가)*다중과업 모델의 비중 : 2022년 41'),\n",
       " Document(id='27ae028e-7e5a-41df-8912-16f9c3a47553', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 18, 'page_label': '19'}, page_content='.§봉강호(2024). “글로벌 초거대 AI 모델 현황 분석(2020~2023년)”, 이슈리포트 IS-178, 성남: 소프트웨어정책연구소.§안성원·유재흥·조원영·노재원·손효현(2023). “초거대언어모델의 부상과 주요이슈 \\xad ChatGPT의 기술적 특징과 사회적·산업적 시사점”, 이슈리포트 IS-158, 성남: 소프트웨어정책연구소.2.\\x00웹사이트§EPOCH AI, “Large-Scale AI Models”, Retrieved from https://epochai.org/data/large-scale-ai-models, (Accessed 6 February 2025)'),\n",
       " Document(id='95d2fb3d-c030-4e6e-8026-146819597cb8', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content=': 중복은 타 국가와 공동개발(합작)한 모델이 중복 계산된 개수를 의미함 <표\\x001>\\x00국가별·연도별\\x00초거대\\x00AI\\x00모델\\x00개발\\x00현황(2020~2024년)(단위: 개) ㅇ2024년까지 출시된 우리나라의 초거대 AI 모델은 총 14개로 조사됨\\xadLG, 삼성, 네이버, KT, NC소프트, 코난테크놀로지 등의 국내 기업이 자체 개발한 모델임\\xad우리나라의 초거대 AI 모델은 언어(Language) 모델이 주를 이루고 있으며, 이 외에 이미지 생성(Image Generation), 비전(Vision), 바이오(Biology) 등의 모델이 있음'),\n",
       " Document(id='3c13af99-16f8-482a-bde0-959f410f60d4', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 17, 'page_label': '18'}, page_content=': 미국(2020년 1월(Google), 5월(OpenAI)) → 이스라엘(2021년 8월(AI21 Labs)) → 한국(2021년 9월(네이버)) → 중국(2021년 10월(Inspur)) 등*초거대 AI 모델 출시 현황 : 2022년 29개 → 2023년 109개 → 2024년 122개\\xad이에 따라 핵심 AI 주체인 산업계의 투자, R&D, 그리고 기업가적 활동으로부터 혁신을 창출함으로써 국가 성장을 도모해야 할 것이며, 이들 산업계의 혁신활동을 이끌어내기 위해서는 국가 차원의 투자를 확대하고 정책적·제도적 지원을 강화할 필요가 있다고 사료됨*초거대 AI 모델은 대부분 산업계(민간)에서 개발되고 있으며, 그 비중이 매년 증가해온 것으로 나타남(2022년 89'),\n",
       " Document(id='69952fa7-f048-4feb-afb0-cde43a53330a', metadata={'producer': 'Hancom PDF 1.3.0.550', 'creator': 'Hwp 2018 10.0.0.14515', 'creationdate': '2025-04-28T10:13:10+09:00', 'author': 'Sohyeon', 'moddate': '2025-04-28T10:13:10+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/RE-189. 2024년 국내외 인공지능 산업 동향 연구.pdf', 'total_pages': 367, 'page': 332, 'page_label': '333'}, page_content='.1.주요 이슈와 향후 전망1) 생성 AI와 초거대 AI 모델의 확산생성 AI 기술과 초거대 AI 모델(오픈AI의 GPT, 구글의 Gemini, 메타의 LLaMA 등)이 다양한 산업에서 빠르게 확산되고 있다. 이러한 모델들은 자연어 처리, 이미지 생성, 추천 시스템 등에서 뛰어난 성능을 발휘하며, 기존 기술의 한계를 넘어선 응용 가능성을 보여주고 있다. 특히, 마이크로소프트와 메타, 구글과 같은 빅테크 기업들은 초거대 AI 모델을 기반으로 맞춤형 서비스를 제공하며 시장 점유율을 확대하고 있다. 2024년에서 초거대 AI 모델의 상업적 활용이 의료(진단 보조), 교육(개인화 학습 플랫폼), 금융(위험 관리와 예측 분석) 등 핵심 분야에서 주목받았다'),\n",
       " Document(id='65587bf3-be19-4c6b-ad2c-1303608462a4', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content=', 즉 ‘스케일링 법칙(scaling law)’에 따라 AI 모델의 대형화 및 대형 모델 개발이 활발하게 이루어지고 있음□연구 내용 및 방법ㅇ본 연구에서는 2020년부터 최근(2024년 12월)까지 전 세계에 출시된 초거대 AI 모델 현황을 분석하여 글로벌 기술 동향과 트렌드를 살펴봄\\xad2020년부터 2024년까지 출시된 초거대 AI 모델에 대해 출시년도(release year), 국가(country), 분야(domain), 과업유형(task), 개발형태(단독/합작), 개발조직 유형(산/학/연) 등의 다양한 기준으로 정리하고 분석함ㅇ추가적으로, 2024년 초거대 AI 모델을 출시한 주요 기업/기관 및 주요 모델에 대해서도 정리함\\xad여기서 주요 기업/기관은 2024년 출시한 모델 수가 3개 이상인 경우를 의미하고'),\n",
       " Document(id='b69a74cc-e7cb-4644-b6b3-fd4787020983', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='.5 7.8BLG AI연구원언어(Language)2024EXAONE 3.5 32BLG AI연구원언어(Language)*자료: EPOCH AI의 데이터를 활용하여 연구자가 작성함 <표\\x002>\\x002020~2024년\\x00출시된\\x00우리나라\\x00초거대\\x00AI\\x00모델\\x00현황 □분야5)별 현황ㅇ2020~2024년 출시된 초거대 AI 모델의 대부분은 언어모델인 것으로 나타났으며, 그 다음 시각, 음성, 바이오 순으로 많았음\\xad언어모델(멀티모달 모델 포함)은 240개로 전체의 88'),\n",
       " Document(id='b0543864-a345-4535-ba1d-c6c888ed249a', metadata={'producer': 'Hancom PDF 1.3.0.550', 'creator': 'Hwp 2018 10.0.0.14515', 'creationdate': '2025-04-28T09:46:58+09:00', 'author': 'Sohyeon', 'moddate': '2025-04-28T09:46:58+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/RE-185. 인공지능 기술·산업 생태계 육성방안 연구.pdf', 'total_pages': 122, 'page': 91, 'page_label': '92'}, page_content='. 특히 멀티모달에 해당하는 초거대 AI 모델은 2021년 처음 등장해 2023년 크게 늘어난 것으로 확인되었다(봉강호, 2025).< 분야별 현황 > < 멀티모달 모델 현황 >(단위: 개; 중복 포함) (단위: 개, %)분야20202021202220232024언어292493112시각162630음성143바이오112로보틱스1수학 2 * 자료: 봉강호(2025)[그림 5-2] 글로벌 초거대 AI 모델 출시 현황(2020~2024년)고도화된 AI에 대한 수요자의 인식은 대체로 긍정적인 상황이다. Gartner(2024)에서 전 세계 CIO 및 기술리더 608명을 대상으로 조사한 바에 따르면, 응답자 중 77%는 AI에 대한 투자를 통해 생산성·효율성 향상이 기대된다고 하였다'),\n",
       " Document(id='18c878ba-6c93-4f0b-99ba-1e4d1202f3c1', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='SPRi\\x00이슈리포트\\x00IS-198글로벌\\x00초거대\\x00AI\\x00모델\\x00현황\\x00분석(2024년\\x00조사) 6 분야2020년2021년2022년2023년2024년계언어(Language)292493112240시각(Vision/Video/Image Generation)16263063음성(Audio/Speech)1438바이오(Biology)1124로보틱스(Robotics)1 1수학(Math) 22 <표\\x003>\\x00분야별\\x00초거대\\x00AI\\x00모델\\x00현황(2020~2024년)(단위: 개; 중복 포함) ㅇ멀티모달(multimodal)6)에 해당하는 초거대 AI 모델은 2024년까지 총 39개가 출시되었으며, 매년 늘어나고 있음\\xad2024년에는 총 23개의 초거대 AI 멀티모달 모델이 출시되었으며'),\n",
       " Document(id='436adc31-8b75-4eb9-b7b2-aa18c414da8a', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='.5%(160개)의 비중을 차지하는 것으로 나타남ㅇ2020~2024년 출시된 초거대 AI 모델이 수행 가능한 과업유형 중 ‘언어 모델링/생성’이 가장 많은 것으로 나타남(208개 모델이 해당)\\xad그 다음으로 ‘채팅(98개)’, ‘코드 생성/자동완성(78개)’'),\n",
       " Document(id='1bc6ba98-692f-4c78-922d-17777dc17718', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 15, 'page_label': '16'}, page_content='.\\x00주요\\x00내용\\x00요약\\x00□글로벌 초거대 AI 모델 개발 트렌드ㅇ2020~2024년 기간에 전 세계적으로 총 271개(누적)의 초거대 AI 모델이 출시되었으며, 매년 증가하는 추세임\\xad2024년에는 전년 대비 13개가 늘어난 122개의 초거대 AI 모델이 출시된 것으로 확인됨ㅇ그간 출시된 초거대 AI 모델 중 대부분은 ‘언어’ 모델이며, 수행 가능한 과업으로는 ‘언어 모델링/생성’인 경우가 가장 많았음ㅇ2024년에는 ‘비전’ 모델과 ‘영상’ 모델 출시가 전년 대비 늘어났으며, 초거대 AI 추론 모델이 처음 등장함ㅇ2024년 ‘코드 생성/자동완성’, ‘언어 모델링/생성’, ‘정량적 추론’ 및 ‘질의응답’ 등을 수행할 수 있는 모델이 상대적으로 많이 출시됨ㅇ초거대 AI 모델 중 멀티모달을 지원하는 모델'),\n",
       " Document(id='34bfd8e6-a352-43d9-9c94-cb7c9b8204c5', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='SPRi\\x00이슈리포트\\x00IS-198글로벌\\x00초거대\\x00AI\\x00모델\\x00현황\\x00분석(2024년\\x00조사) 9 □개발 형태 및 조직 유형별 현황ㅇ2020~2024년 출시된 초거대 AI 모델은 대부분 단일 조직이 개발(단독 개발)한 것으로 나타남\\xad전체 271개 모델 중 단독 개발 모델은 249개(91.9%), 복수 조직(기업)이 합작 개발한 모델은 27개(10.0%)로 확인됨 [그림\\x004]\\x00개발\\x00방식별\\x00초거대\\x00AI\\x00모델\\x00현황(2020년~2024년\\x00누적)ㅇ2020~2024년 출시된 초거대 AI 모델 개발조직은 대부분 산업계(기업)에 해당하는 것으로 나타남\\xad전체 271개 모델 중 257개(94'),\n",
       " Document(id='555e840a-b9c1-46cd-a368-c797a1a01e7e', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='SPRi\\x00이슈리포트\\x00IS-198글로벌\\x00초거대\\x00AI\\x00모델\\x00현황\\x00분석(2024년\\x00조사) 7 □과업(task)유형별 현황ㅇ2020~2024년 출시된 271개 초거대 AI 모델 중 과업유형 관련 정보가 확인되는 모델은 총 269개였으며, 이 중 복수의 과업 수행능력을 보유한 AI 모델은 59'),\n",
       " Document(id='92b8d9fe-12fc-4912-b017-19becf7bb68d', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='SPRi\\x00이슈리포트\\x00IS-198글로벌\\x00초거대\\x00AI\\x00모델\\x00현황\\x00분석(2024년\\x00조사) 2 \\xadEPOCH AI는 美 스탠포드대학교 인간중심인공지능연구소(Stanford HAI)에서 발간하는 「AI Index」에서 ‘주목할 만한 머신러닝 모델(Notable Machine Learning Model)’ 현황의 원자료 제공 기관(출처)임\\xadEPOCH AI의 DB에는 벤치마크 리더보드(허깅페이스, PWC, 美 스탠포드대학교 CRFM), 각국의 주요 검색엔진, 주요 연구기관, 연구문헌 등을 모니터링하여 확인되는 초거대 AI 모델 정보가 수록됨*단, 정보 수집방법의 한계로 인해 일부 모델(개발주체가 기밀 유지를 위해 발표/공개하지 않은 모델'),\n",
       " Document(id='0312d556-a32d-4374-a82e-fd293b86f691', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='SPRi\\x00이슈리포트\\x00IS-198글로벌\\x00초거대\\x00AI\\x00모델\\x00현황\\x00분석(2024년\\x00조사) 11 ㅇ2024년에 출시된 초거대 AI 모델 중 학습 연산량(FLOP)을 기준으로 상위 5개의 모델을 선정하였으며, 가장 큰 규모의 모델은 xAI에서 출시한 Grok-2로 확인됨\\xad상위 5개 모델의 공통점은 기업(산업계)에서 개발하여 출시한 AI라는 것임모델명Grok-2GPT-4oLlama 3.1-405BClaud 3'),\n",
       " Document(id='7b343f67-3764-428b-8f49-d2c934745de5', metadata={'producer': 'Hancom PDF 1.3.0.550', 'creator': 'Hwp 2018 10.0.0.14515', 'creationdate': '2025-04-28T09:46:58+09:00', 'author': 'Sohyeon', 'moddate': '2025-04-28T09:46:58+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/RE-185. 인공지능 기술·산업 생태계 육성방안 연구.pdf', 'total_pages': 122, 'page': 91, 'page_label': '92'}, page_content='- 83 여기에 더해, 대용량 연산 인프라를 바탕으로 방대한 데이터를 학습해 우수한 성능을 보이는 초거대 AI가 등장하고, 그리고 AI가 수행가능한 기능(task)이 다양해짐에 따라 AI의 활용도가 더욱 높아졌다. 특히 텍스트(언어) 외에도 음성, 이미지, 비디오, 생체신호(바이오) 등의 여러 데이터 유형을 처리·이해·생성할 수 있는 ‘멀티모달(multimodal) AI’가 등장하면서 AI가 적용·응용될 수 있는 분야/영역이 대폭 늘어나게 되었다. 지난 2020년부터 2024년 사이 전 세계에 출시된 초거대 AI 모델 현황을 분석한 결과에 따르면, 2020년 출시된 초거대 AI 모델은 모두 언어모델이었으나 2021년부터 시각, 음성, 바이오, 로보틱스 등 다양한 초거대 AI 모델이 출시된 것으로 나타났다'),\n",
       " Document(id='6ae5db84-cd59-4041-bb2d-e60f4949a86f', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='.5%임(단위: 개) [그림\\x001]\\x00연도별\\x00초거대\\x00AI\\x00모델\\x00출시\\x00건수□국가별 현황ㅇ2020~2024년 기간에 초거대 AI 모델을 가장 많이 개발한 국가는 미국(128개)으로 나타났으며, 그 다음 중국(95개), 한국(14개), 프랑스(10개) 순으로 많았음ㅇ2024년 단일년도 기준으로 보면 미국(63개), 중국(45개), 한국·프랑스(3개) 순으로 초거대 AI 모델을 많이 출시함'),\n",
       " Document(id='dbc6c3f8-c6a3-4caf-800e-56def8286c36', metadata={'producer': 'Hancom PDF 1.3.0.547', 'creator': 'Hwp 2024 13.0.0.711', 'creationdate': '2025-02-14T14:24:06+09:00', 'author': 'Kevin', 'moddate': '2025-02-14T14:24:06+09:00', 'pdfversion': '1.4', 'source': '../data/pdf/IS-198 글로벌 초거대 AI 모델 현황 분석(2024년 조사).pdf', 'total_pages': 21, 'page': 15, 'page_label': '16'}, page_content='.7%, 2023년 93.6%, 2024년 96.7%로 매년 증가함ㅇ2024년 초거대 AI 모델을 가장 많이 출시한 기업/기관은 중국 기업 Alibaba이며, 초거대 AI 모델을 3개 이상 출시한 주요 기업/기관 중에서는 미국 국적인 경우가 절반을 차지하며 가장 많은 것으로 나타남–2024년 주요 기업/기관은 미국, 중국, 한국, 프랑스 국적 순으로 많았음')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"초거대 AI 모델이란?\", k=20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
