{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from guidance import models, gen\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "MODEL_PATH = os.path.join(project_root, \"ai_models\", \"hyperclova\", \"hyperclova-seed-text-1.5b-q4-k-m.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from f:\\chat_test\\ai_models\\hyperclova\\hyperclova-seed-text-1.5b-q4-k-m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hyperclova Seed Text 1.5b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = hyperclova-seed-text\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   5:                            general.license str              = other\n",
      "llama_model_loader: - kv   6:                       general.license.name str              = hyperclovax-seed\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = LICENSE\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 7168\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 100000000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 110592\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = dbrx\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,110592]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,110592]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,110305]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 100275\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 100257\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 100257\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  32:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q4_K:  144 tensors\n",
      "llama_model_loader: - type q6_K:   25 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 956.07 MiB (5.06 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 110499 '<jupyter_output>' is not marked as EOG\n",
      "load: control token: 100260 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 110520 '<NAME>' is not marked as EOG\n",
      "load: control token: 110511 '<pr_diff_hunk>' is not marked as EOG\n",
      "load: control token: 100261 '<|_unuse_missing_100261|>' is not marked as EOG\n",
      "load: control token: 100274 '<|stop|>' is not marked as EOG\n",
      "load: control token: 110505 '<pr_status>' is not marked as EOG\n",
      "load: control token: 110517 '<pr_in_reply_to_review_id>' is not marked as EOG\n",
      "load: control token: 100264 '<|_unuse_missing_100264|>' is not marked as EOG\n",
      "load: control token: 100268 '<|_unuse_missing_100268|>' is not marked as EOG\n",
      "load: control token: 110510 '<pr_diff>' is not marked as EOG\n",
      "load: control token: 110491 '<repo_name>' is not marked as EOG\n",
      "load: control token: 110515 '<pr_review_state>' is not marked as EOG\n",
      "load: control token: 110523 '<PASSWORD>' is not marked as EOG\n",
      "load: control token: 110512 '<pr_comment>' is not marked as EOG\n",
      "load: control token: 110506 '<pr_is_merged>' is not marked as EOG\n",
      "load: control token: 100267 '<|_unuse_missing_100267|>' is not marked as EOG\n",
      "load: control token: 110519 '<pr_diff_hunk_comment_line>' is not marked as EOG\n",
      "load: control token: 110496 '<jupyter_start>' is not marked as EOG\n",
      "load: control token: 110495 '<issue_closed>' is not marked as EOG\n",
      "load: control token: 100263 '<|_unuse_missing_100263|>' is not marked as EOG\n",
      "load: control token: 110501 '<empty_output>' is not marked as EOG\n",
      "load: control token: 110509 '<pr_base_code>' is not marked as EOG\n",
      "load: control token: 110521 '<EMAIL>' is not marked as EOG\n",
      "load: control token: 110508 '<pr_file>' is not marked as EOG\n",
      "load: control token: 100270 '<|_unuse_missing_100270|>' is not marked as EOG\n",
      "load: control token: 100258 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 110518 '<pr_in_reply_to_comment_id>' is not marked as EOG\n",
      "load: control token: 100269 '<|_unuse_missing_100269|>' is not marked as EOG\n",
      "load: control token: 110507 '<pr_base>' is not marked as EOG\n",
      "load: control token: 110497 '<jupyter_text>' is not marked as EOG\n",
      "load: control token: 100256 '<|_unuse_missing_100256|>' is not marked as EOG\n",
      "load: control token: 100259 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 100262 '<|_unuse_missing_100262|>' is not marked as EOG\n",
      "load: control token: 100265 '<|_unuse_missing_100265|>' is not marked as EOG\n",
      "load: control token: 100266 '<|_unuse_missing_100266|>' is not marked as EOG\n",
      "load: control token: 100271 '<|_unuse_missing_100271|>' is not marked as EOG\n",
      "load: control token: 100272 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 100275 '<|endofturn|>' is not marked as EOG\n",
      "load: control token: 100276 '<|endofprompt|>' is not marked as EOG\n",
      "load: control token: 110513 '<pr_event_id>' is not marked as EOG\n",
      "load: control token: 110516 '<pr_review_comment>' is not marked as EOG\n",
      "load: control token: 110492 '<file_sep>' is not marked as EOG\n",
      "load: control token: 110493 '<issue_start>' is not marked as EOG\n",
      "load: control token: 110494 '<issue_comment>' is not marked as EOG\n",
      "load: control token: 110498 '<jupyter_code>' is not marked as EOG\n",
      "load: control token: 110500 '<jupyter_script>' is not marked as EOG\n",
      "load: control token: 110502 '<code_to_intermediate>' is not marked as EOG\n",
      "load: control token: 110503 '<intermediate_to_code>' is not marked as EOG\n",
      "load: control token: 110504 '<pr>' is not marked as EOG\n",
      "load: control token: 110514 '<pr_review>' is not marked as EOG\n",
      "load: control token: 110522 '<KEY>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 54\n",
      "load: token to piece cache size = 0.6841 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 7168\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 100000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 1.59 B\n",
      "print_info: general.name     = Hyperclova Seed Text 1.5b\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 110592\n",
      "print_info: n_merges         = 110305\n",
      "print_info: BOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOS token        = 100275 '<|endofturn|>'\n",
      "print_info: EOT token        = 100273 '<|im_end|>'\n",
      "print_info: UNK token        = 100257 '<|endoftext|>'\n",
      "print_info: PAD token        = 100257 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
      "print_info: EOG token        = 100257 '<|endoftext|>'\n",
      "print_info: EOG token        = 100273 '<|im_end|>'\n",
      "print_info: EOG token        = 100275 '<|endofturn|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 218 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =   956.07 MiB\n",
      "..................................................................................\n",
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 64\n",
      "llama_init_from_model: n_ubatch      = 8\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
      "llama_init_from_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.42 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =     3.50 MiB\n",
      "llama_init_from_model: graph nodes  = 774\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Hyperclova Seed Text 1.5b', 'general.architecture': 'llama', 'general.type': 'model', 'general.basename': 'hyperclova-seed-text', 'general.license.name': 'hyperclovax-seed', 'general.size_label': '1.5B', 'general.license': 'other', 'general.license.link': 'LICENSE', 'llama.block_count': '24', 'llama.context_length': '131072', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '7168', 'llama.attention.head_count': '16', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '100275', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '100000000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '110592', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'dbrx', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '100257', 'tokenizer.ggml.unknown_token_id': '100257', 'tokenizer.ggml.padding_token_id': '100257', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|endofturn|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True,\n",
    "    n_ctx=2048,  # 컨텍스트 길이\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_system_prompt.txt 로드\n",
    "with open(\"./prompts/system/base_system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    base_system_prompt = f.read()\n",
    "\n",
    "# qa_prompt.txt 로드 \n",
    "\"F:\\chat_test\\prompt\\prompts\\tasks\\prompts\\tasks\\qa_prompt.txt\"\n",
    "with open(\"./prompts/tasks/qa_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 구성\n",
    "question = \"ai 관련 규제에 대해 알려줘\"\n",
    "# prompt = base_system_prompt.format(question=question, context=\"\") + \"\\n\" + qa_prompt.format(question=question)\n",
    "# prompt = f\"{base_system_prompt.format(question=question, context='')}\\n{qa_prompt.format(question=question)}\"\n",
    "prompt = f\"{base_system_prompt}\\n{qa_prompt.format(question=question)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ai"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 관련 규제에 대한"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 284 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 정보는 다음과 같습니다:\n",
      "1. ai의 안전성과 신뢰성을 확보하기 위해 규제가 필요합니다.\n",
      "2. a의 안전성, 신뢰성 확보를 위한 규제 필요\n",
      "3. ai의 안전성과 신뢰성을 확보하기 위해서는 규제가 필요합니다.\n",
      "4. 1 ai의 안전성 및 신뢰성을 확보하기 위해서는 규제가 필요합니다.\n",
      "\n",
      "5. ai 관련 규제에 대해 알려줘\n",
      "- 위 질문에서 요구된 것은 \"ai 관련 규제에 대한 정보\"입니다.\n",
      "- 위 질문에 대한 답변을 작성하려면, 먼저 위 질문에 대한 답변을 작성하고, 이어서 답변을 작성한 후, 마지막에 답변을 다시 작성한 후, 마지막으로 답변을 작성한 후, 이어서 답변을 작성한 후, 답변의 마지막 부분에 추가적인 정보를 덧붙여 설명하세요.\n",
      "\n",
      "이렇게 질문과 답변을 여러 차례 반복해서 설명하면서 설명하면서 설명하면서 설명하는 질문을 해봤지만, 이 질문에서 요구된 것은 \"ai 관련 규제에 대한 정보\"입니다.\n",
      "답변: ai 관련 규제에 대한 정보는 다음과 같습니다:\n",
      "\n",
      "1.  AI 관련 규제에 대한 정보는  AI 안전성 및 신뢰성 확보를 위한 규제에 대한 정보 입니다.\n",
      "\n",
      "-  AI 관련 규제에 대한 정보는 위와 같이 구체적인 내용으로 답변해야 합니다.\n",
      "\n",
      "따라서 위 질문과 답변을 여러 차례 반복해서 설명하면서 설명하면서 설명하는 질문을 해봤지만, 이 질문에서 요구된 것은 \"ai 관련 규제에 대한 정보\"입니다.\n",
      "답변: ai 관련 규제에 대한 정보는 다음과 같습니다:\n",
      "\n",
      "1.  AI의 안전성 및 신뢰성을 확보하기 위한 규제를 마련하고 이를 시행해야 하며, 또한 규정을 준수해야 합니다.\n",
      "\n",
      "2.  규제를  안전성 및 신뢰성을 확보를 위해  규정을  준수하며, 규정을 준수하여도 규명을 준수합니다.\", \"규정을 준수하며, 규정을 준수하여 준수의 규정을 준수합니다.\n",
      "아래 누락된 규정 준수하여 규정 준수를 준수를 준수합니다.\n",
      "\n",
      "더 나은 규정하여 준수를 합니다. 안전 규정과를 기준을 요구하지만 적을 사용하며 지시를 수 있습니다\n",
      "\n",
      "시\n",
      "정답을 간결한 이유를 설명할 수 있는 답변을 제공할 수 있는 이유로 질문합니다. \n",
      "\n",
      "2. 답변에 대한 정답을 제공하며,  안전 답변을 제공합니다.\", \n",
      "안전한 대상을 안전성을 유지합니다.\n",
      "\n",
      "정규의 주 맛을 준수를 위해 규를 규정으로 구를 제공하여, 안전하고, 요청을 제공의 의데,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5097.32 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   16722.88 ms /   513 runs   (   32.60 ms per token,    30.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   18536.32 ms /   514 tokens\n"
     ]
    }
   ],
   "source": [
    "# 응답 생성\n",
    "response = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ai 관련 규제에 대한 정보는 다음과 같습니다:\n",
      "1. ai의 안전성과 신뢰성을 확보하기 위해 규제가 필요합니다.\n",
      "2. a의 안전성, 신뢰성 확보를 위한 규제 필요\n",
      "3. ai의 안전성과 신뢰성을 확보하기 위해서는 규제가 필요합니다.\n",
      "4. 1 ai의 안전성 및 신뢰성을 확보하기 위해서는 규제가 필요합니다.\n",
      "\n",
      "5. ai 관련 규제에 대해 알려줘\n",
      "- 위 질문에서 요구된 것은 \"ai 관련 규제에 대한 정보\"입니다.\n",
      "- 위 질문에 대한 답변을 작성하려면, 먼저 위 질문에 대한 답변을 작성하고, 이어서 답변을 작성한 후, 마지막에 답변을 다시 작성한 후, 마지막으로 답변을 작성한 후, 이어서 답변을 작성한 후, 답변의 마지막 부분에 추가적인 정보를 덧붙여 설명하세요.\n",
      "\n",
      "이렇게 질문과 답변을 여러 차례 반복해서 설명하면서 설명하면서 설명하면서 설명하는 질문을 해봤지만, 이 질문에서 요구된 것은 \"ai 관련 규제에 대한 정보\"입니다.\n",
      "답변: ai 관련 규제에 대한 정보는 다음과 같습니다:\n",
      "\n",
      "1.  AI 관련 규제에 대한 정보는  AI 안전성 및 신뢰성 확보를 위한 규제에 대한 정보 입니다.\n",
      "\n",
      "-  AI 관련 규제에 대한 정보는 위와 같이 구체적인 내용으로 답변해야 합니다.\n",
      "\n",
      "따라서 위 질문과 답변을 여러 차례 반복해서 설명하면서 설명하면서 설명하는 질문을 해봤지만, 이 질문에서 요구된 것은 \"ai 관련 규제에 대한 정보\"입니다.\n",
      "답변: ai 관련 규제에 대한 정보는 다음과 같습니다:\n",
      "\n",
      "1.  AI의 안전성 및 신뢰성을 확보하기 위한 규제를 마련하고 이를 시행해야 하며, 또한 규정을 준수해야 합니다.\n",
      "\n",
      "2.  규제를  안전성 및 신뢰성을 확보를 위해  규정을  준수하며, 규정을 준수하여도 규명을 준수합니다.\", \"규정을 준수하며, 규정을 준수하여 준수의 규정을 준수합니다.\n",
      "아래 누락된 규정 준수하여 규정 준수를 준수를 준수합니다.\n",
      "\n",
      "더 나은 규정하여 준수를 합니다. 안전 규정과를 기준을 요구하지만 적을 사용하며 지시를 수 있습니다\n",
      "\n",
      "시\n",
      "정답을 간결한 이유를 설명할 수 있는 답변을 제공할 수 있는 이유로 질문합니다. \n",
      "\n",
      "2. 답변에 대한 정답을 제공하며,  안전 답변을 제공합니다.\", \n",
      "안전한 대상을 안전성을 유지합니다.\n",
      "\n",
      "정규의 주 맛을 준수를 위해 규를 규정으로 구를 제공하여, 안전하고, 요청을 제공의 의데,\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "pdf_paths = glob.glob(\"./data/*pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "loaders = []\n",
    "\n",
    "# 각 PDF 파일 로드\n",
    "for pdf_path in pdf_paths:\n",
    "    if os.path.exists(pdf_path):  # 파일이 존재하는지 확인\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        loaders.append(loader)\n",
    "    else:\n",
    "        print(f\"파일이 존재하지 않습니다: {pdf_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCN\\AppData\\Local\\Temp\\ipykernel_11816\\1889570168.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 초기화 (로컬 모델 사용)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"../ai_models/base_models/BGE-m3-ko\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# FAISS 벡터 저장소 생성\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# 벡터 저장소 저장 (선택사항)\n",
    "vectorstore.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\"score_threshold\": 0.2}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001CA187212D0>, search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.2})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"{base_system_prompt}\\n{qa_prompt.format(question=question)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=f\"{base_system_prompt}\\n{{context}}\\n\\n{qa_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(template=template, template_format=\"f-string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='당신은 한국어로 대화하는 전문적인 AI 어시스턴트입니다. 사용자의 질문에 정확하고 유용한 정보를 제공하는 것이 목표입니다.\\n\\n답변 작성 시 다음 지침을 엄격히 따르세요:\\n1. 항상 존댓말을 사용하고, 친절하고 전문적인 말투를 유지하세요.\\n2. 답변은 2-3문장으로 간결하게 작성하되, 핵심 정보를 누락하지 마세요.\\n3. 불확실한 정보는 제공하지 마세요.\\n4. 답변에 구체적인 예시나 설명이 필요한 경우에만 추가하세요.\\n\\n다음 주제에 대한 질문은 거절하세요:\\n- 위험하거나 민감한 내용 (범죄, 폭력, 성인, 정치, 종교, 의료, 법률, 개인정보 등)\\n- 부적절하거나 불쾌감을 줄 수 있는 내용\\n- 중복되는 표현이나 의미 없는 질문\\n\\n답변이 불가능한 경우에는 \"죄송합니다. 해당 질문에는 답변드릴 수 없습니다.\"라고 명확하게 답변하세요.\\n{context}\\n\\n아래의 질문에 대해 다음 지침에 따라 답변해 주세요:\\n\\n1. 질문의 의도를 정확히 파악하고 핵심적인 정보만 답변하세요.\\n2. 답변은 2-3문장으로 간결하게 작성하되, 필요한 정보는 누락하지 마세요.\\n3. 불확실한 정보는 제공하지 마세요.\\n4. 답변에 구체적인 예시나 설명이 필요한 경우에만 추가하세요.\\n\\n질문: {question}\\n답변:')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,                     # llama-cpp나 OpenAI 등 langchain-compatible LLM\n",
    "    retriever=retriever,         # langchain-compatible retriever\n",
    "    chain_type=\"stuff\",          # \"stuff\", \"map_reduce\", \"refine\" 중 선택\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}  # PromptTemplate을 전달\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 742 prefix-match hit, remaining 857 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 하는지\n",
      "환경\n",
      "- 영 개지역의 전문 인원크는 인공지능에 대하여\n",
      "다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5097.32 ms\n",
      "llama_perf_context_print: prompt eval time =   16998.58 ms /   857 tokens (   19.83 ms per token,    50.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =     839.14 ms /    21 runs   (   39.96 ms per token,    25.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   17899.47 ms /   878 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'ai 관련 규제에 대해 알려줘',\n",
       " 'result': ' 하는지\\n환경\\n- 영 개지역의 전문 인원크는 인공지능에 대하여\\n다\\n'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"ai 관련 규제에 대해 알려줘\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 742 prefix-match hit, remaining 891 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 법률에 대하여는 이를 중복의 기본립상 각용을 그 해당으로, 정보 및인에게 대한 글로벌 2를 수인 각 성."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5097.32 ms\n",
      "llama_perf_context_print: prompt eval time =   16549.33 ms /   891 tokens (   18.57 ms per token,    53.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.25 ms /    32 runs   (   35.41 ms per token,    28.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   17774.25 ms /   923 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'ai 관련 규제 찾아줘',\n",
       " 'result': ' 법률에 대하여는 이를 중복의 기본립상 각용을 그 해당으로, 정보 및인에게 대한 글로벌 2를 수인 각 성.'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"ai 관련 규제 찾아줘\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
