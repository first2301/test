{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "MODEL_PATH = os.path.join(project_root, \"ai_models\", \"hyperclova\", \"hyperclova-seed-text-1.5b-q4-k-m.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_system_prompt.txt 로드\n",
    "with open(\"./prompts/system/base_system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    base_system_prompt = f.read()\n",
    "\n",
    "# qa_prompt.txt 로드 \n",
    "\"F:\\chat_test\\prompt\\prompts\\tasks\\prompts\\tasks\\qa_prompt.txt\"\n",
    "with open(\"./prompts/tasks/qa_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCN\\AppData\\Local\\Temp\\ipykernel_17212\\3227459308.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"../ai_models/base_models/BGE-m3-ko\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\"./faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\"score_threshold\": 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from f:\\chat_test\\ai_models\\hyperclova\\hyperclova-seed-text-1.5b-q4-k-m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hyperclova Seed Text 1.5b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = hyperclova-seed-text\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   5:                            general.license str              = other\n",
      "llama_model_loader: - kv   6:                       general.license.name str              = hyperclovax-seed\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = LICENSE\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 7168\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 100000000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 110592\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = dbrx\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,110592]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,110592]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,110305]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 100275\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 100257\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 100257\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  32:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q4_K:  144 tensors\n",
      "llama_model_loader: - type q6_K:   25 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 956.07 MiB (5.06 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 110499 '<jupyter_output>' is not marked as EOG\n",
      "load: control token: 100260 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 110520 '<NAME>' is not marked as EOG\n",
      "load: control token: 110511 '<pr_diff_hunk>' is not marked as EOG\n",
      "load: control token: 100261 '<|_unuse_missing_100261|>' is not marked as EOG\n",
      "load: control token: 100274 '<|stop|>' is not marked as EOG\n",
      "load: control token: 110505 '<pr_status>' is not marked as EOG\n",
      "load: control token: 110517 '<pr_in_reply_to_review_id>' is not marked as EOG\n",
      "load: control token: 100264 '<|_unuse_missing_100264|>' is not marked as EOG\n",
      "load: control token: 100268 '<|_unuse_missing_100268|>' is not marked as EOG\n",
      "load: control token: 110510 '<pr_diff>' is not marked as EOG\n",
      "load: control token: 110491 '<repo_name>' is not marked as EOG\n",
      "load: control token: 110515 '<pr_review_state>' is not marked as EOG\n",
      "load: control token: 110523 '<PASSWORD>' is not marked as EOG\n",
      "load: control token: 110512 '<pr_comment>' is not marked as EOG\n",
      "load: control token: 110506 '<pr_is_merged>' is not marked as EOG\n",
      "load: control token: 100267 '<|_unuse_missing_100267|>' is not marked as EOG\n",
      "load: control token: 110519 '<pr_diff_hunk_comment_line>' is not marked as EOG\n",
      "load: control token: 110496 '<jupyter_start>' is not marked as EOG\n",
      "load: control token: 110495 '<issue_closed>' is not marked as EOG\n",
      "load: control token: 100263 '<|_unuse_missing_100263|>' is not marked as EOG\n",
      "load: control token: 110501 '<empty_output>' is not marked as EOG\n",
      "load: control token: 110509 '<pr_base_code>' is not marked as EOG\n",
      "load: control token: 110521 '<EMAIL>' is not marked as EOG\n",
      "load: control token: 110508 '<pr_file>' is not marked as EOG\n",
      "load: control token: 100270 '<|_unuse_missing_100270|>' is not marked as EOG\n",
      "load: control token: 100258 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 110518 '<pr_in_reply_to_comment_id>' is not marked as EOG\n",
      "load: control token: 100269 '<|_unuse_missing_100269|>' is not marked as EOG\n",
      "load: control token: 110507 '<pr_base>' is not marked as EOG\n",
      "load: control token: 110497 '<jupyter_text>' is not marked as EOG\n",
      "load: control token: 100256 '<|_unuse_missing_100256|>' is not marked as EOG\n",
      "load: control token: 100259 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 100262 '<|_unuse_missing_100262|>' is not marked as EOG\n",
      "load: control token: 100265 '<|_unuse_missing_100265|>' is not marked as EOG\n",
      "load: control token: 100266 '<|_unuse_missing_100266|>' is not marked as EOG\n",
      "load: control token: 100271 '<|_unuse_missing_100271|>' is not marked as EOG\n",
      "load: control token: 100272 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 100275 '<|endofturn|>' is not marked as EOG\n",
      "load: control token: 100276 '<|endofprompt|>' is not marked as EOG\n",
      "load: control token: 110513 '<pr_event_id>' is not marked as EOG\n",
      "load: control token: 110516 '<pr_review_comment>' is not marked as EOG\n",
      "load: control token: 110492 '<file_sep>' is not marked as EOG\n",
      "load: control token: 110493 '<issue_start>' is not marked as EOG\n",
      "load: control token: 110494 '<issue_comment>' is not marked as EOG\n",
      "load: control token: 110498 '<jupyter_code>' is not marked as EOG\n",
      "load: control token: 110500 '<jupyter_script>' is not marked as EOG\n",
      "load: control token: 110502 '<code_to_intermediate>' is not marked as EOG\n",
      "load: control token: 110503 '<intermediate_to_code>' is not marked as EOG\n",
      "load: control token: 110504 '<pr>' is not marked as EOG\n",
      "load: control token: 110514 '<pr_review>' is not marked as EOG\n",
      "load: control token: 110522 '<KEY>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 54\n",
      "load: token to piece cache size = 0.6841 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 7168\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 100000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 1.59 B\n",
      "print_info: general.name     = Hyperclova Seed Text 1.5b\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 110592\n",
      "print_info: n_merges         = 110305\n",
      "print_info: BOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOS token        = 100275 '<|endofturn|>'\n",
      "print_info: EOT token        = 100273 '<|im_end|>'\n",
      "print_info: UNK token        = 100257 '<|endoftext|>'\n",
      "print_info: PAD token        = 100257 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
      "print_info: EOG token        = 100257 '<|endoftext|>'\n",
      "print_info: EOG token        = 100273 '<|im_end|>'\n",
      "print_info: EOG token        = 100275 '<|endofturn|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 218 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =   956.07 MiB\n",
      "..................................................................................\n",
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 64\n",
      "llama_init_from_model: n_ubatch      = 8\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
      "llama_init_from_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.42 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =     3.50 MiB\n",
      "llama_init_from_model: graph nodes  = 774\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Hyperclova Seed Text 1.5b', 'general.architecture': 'llama', 'general.type': 'model', 'general.basename': 'hyperclova-seed-text', 'general.license.name': 'hyperclovax-seed', 'general.size_label': '1.5B', 'general.license': 'other', 'general.license.link': 'LICENSE', 'llama.block_count': '24', 'llama.context_length': '131072', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '7168', 'llama.attention.head_count': '16', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '100275', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '100000000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '110592', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'dbrx', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '100257', 'tokenizer.ggml.unknown_token_id': '100257', 'tokenizer.ggml.padding_token_id': '100257', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|endofturn|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True,\n",
    "    n_ctx=2048,  # 컨텍스트 길이\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=f\"{base_system_prompt}\\n{{context}}\\n\\n{qa_prompt}\"\n",
    "prompt_template = PromptTemplate.from_template(template=template, template_format=\"f-string\")\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,                     # llama-cpp나 OpenAI 등 langchain-compatible LLM\n",
    "    retriever=retriever,         # langchain-compatible retriever\n",
    "    chain_type=\"stuff\",          # \"stuff\", \"map_reduce\", \"refine\" 중 선택\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}  # PromptTemplate을 전달\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 규제 샌드박스 박스에 대해 알려드리겠습니다.\n",
      "\n",
      "다음은 질문에 대한 답변입니다:\n",
      "- \"규제 샌드박스 박스는 무엇인가요?\"\n",
      "- \"규제 샌드박스 박스에서는 어떤 규제가 이루어지나요?\"\n",
      "\n",
      "규제 샌드박스 박스에 대해 알려드리도록 하겠습니다."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5666.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5665.67 ms /   288 tokens (   19.67 ms per token,    50.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2299.07 ms /    63 runs   (   36.49 ms per token,    27.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    8164.61 ms /   351 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 규제 샌드박스 박스에 대해 알려드리겠습니다.\\n\\n다음은 질문에 대한 답변입니다:\\n- \"규제 샌드박스 박스는 무엇인가요?\"\\n- \"규제 샌드박스 박스에서는 어떤 규제가 이루어지나요?\"\\n\\n규제 샌드박스 박스에 대해 알려드리도록 하겠습니다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"규제샌드박스에 대해 알려줘줘\").get(\"result\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
