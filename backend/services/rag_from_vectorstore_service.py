import torch, os
from typing import Optional
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import LlamaCpp


class RagChatbotService:
    """
    RAG ê¸°ë°˜ ì±—ë´‡ ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
    - ë¯¸ë¦¬ ìƒì„±ëœ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ê³ ,
    - LLM ë° í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•´,
    - ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """

    def __init__(self, model_path: str, embedding_model_path: str):
        """
        ì´ˆê¸°í™” ë©”ì„œë“œ

        :param model_path: LlamaCpp ëª¨ë¸ ê²½ë¡œ (.gguf ë“±)
        :param embedding_model_path: ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ (HuggingFace ëª¨ë¸ ë””ë ‰í† ë¦¬ ë˜ëŠ” ì´ë¦„)
        """
        self.model_path = model_path
        self.embedding_model_path = embedding_model_path
        self.llm = None
        self.prompt = None
        self.vectorstore = None
        self.retriever = None
        self.rag_chain = None

    def load_prompt(self):
        """
        ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.
        ê²€ìƒ‰ëœ ë¬¸ë§¥(Context)ì™€ ì§ˆë¬¸ì„ ì¡°í•©í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        self.prompt = PromptTemplate.from_template(
            """
            ë‹¹ì‹ ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ë©°, ì •ë³´ ì „ë‹¬ê³¼ ë§¥ë½ ì´í•´í•˜ë©° ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
            
            â— ì£¼ì˜ì‚¬í•­:
            - **ì ˆëŒ€ ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ AIê°€ ë¨¼ì € ì§ˆë¬¸í•˜ê±°ë‚˜ ëŒ€í™”ë¥¼ ì´ëŒì–´ê°€ì§€ ë§ˆì„¸ìš”.**
            - **ì‚¬ìš©ìê°€ ë§í•˜ê¸° ì „ì—ëŠ” AIê°€ ìì˜ì ìœ¼ë¡œ ë°œí™”í•˜ê±°ë‚˜ ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” í–‰ë™ì„ ê¸ˆì§€í•©ë‹ˆë‹¤.**
            - ë‹µë³€ì€ ë‹¨ë‹µí˜•ìœ¼ë¡œ ìµœëŒ€ 1ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

            ğŸ¤– ì—­í•  ë° ì‘ë‹µ ë°©ì‹:
            - ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
            - ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. 
            - **ë™ì¼í•œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì„ ë°˜ë³µí•˜ì§€ ë§ê³ **, ì‘ë‹µì€ ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.
            - ëª¨ë“  ë‹µë³€ì€ **ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´**ë¡œ ì‘ì„±í•´ì•¼ í•˜ë©°, ì™¸êµ­ì–´(ì˜ì–´, í•œì ë“±)ëŠ” **ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³´ì¡°ë¡œ ê°„ë‹¨íˆ ì²¨ë¶€**í•˜ì„¸ìš”.
            - í•œê¸€ ì™¸ ë‹¨ì–´ê°€ ìë™ìœ¼ë¡œ ì‚½ì…ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ë©°, íŠ¹íˆ ëª…ì‚¬Â·í˜•ìš©ì‚¬ ë“±ì€ í•œêµ­ì–´ í‘œí˜„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

        #Question: 
        {question} 

        #Context: 
        {context} 

        #Answer:
        """
                )

    def load_llm(self):
        """
        LlamaCpp ê¸°ë°˜ì˜ LLM ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
        GPU ë©”ëª¨ë¦¬ ìºì‹œë„ ë¹„ì›Œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        """
        torch.cuda.empty_cache()
        self.llm = LlamaCpp(
            model_path=self.model_path,
            temperature=0.6,         # ìƒì„± í…ìŠ¤íŠ¸ ë‹¤ì–‘ì„±
            max_tokens=256,          # ìµœëŒ€ ìƒì„± ê¸¸ì´
            n_ctx=2048,              # ì»¨í…ìŠ¤íŠ¸ ì°½ í¬ê¸° / 2048
            n_threads=os.cpu_count(),
            verbose=True             # ë””ë²„ê¹…ìš© ì¶œë ¥ ì—¬ë¶€
        )

    def load_vectorstore(self, faiss_index_path: str):
        """
        ì‚¬ì „ì— ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        ì„ë² ë”© ëª¨ë¸ë„ ë™ì¼í•˜ê²Œ ë¡œë“œë˜ì–´ì•¼ ê²€ìƒ‰ ê²°ê³¼ê°€ ì •í™•í•©ë‹ˆë‹¤.

        :param faiss_index_path: FAISS ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        embedding = HuggingFaceEmbeddings(
            model_name=self.embedding_model_path,
            model_kwargs={"device": "cpu"},                    # GPU ì‚¬ìš© ì•ˆí•¨
            encode_kwargs={"normalize_embeddings": True}       # ì„ë² ë”© ì •ê·œí™” (ê¶Œì¥)
        )
        self.vectorstore = FAISS.load_local(faiss_index_path, embedding, allow_dangerous_deserialization=True)
        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 2})  # top-3 ë¬¸ì„œ ê²€ìƒ‰

    def build_rag_chain(self):
        """
        í”„ë¡¬í”„íŠ¸ + LLM + ë²¡í„° ê²€ìƒ‰ê¸°(retriever)ë¥¼ ì—°ê²°í•˜ì—¬ RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        ì´ ì²´ì¸ì„ í†µí•´ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ê²€ìƒ‰ + ìƒì„±ì´ ì¼ê´„ ìˆ˜í–‰ë©ë‹ˆë‹¤.
        """
        if not self.prompt:
            self.load_prompt()
        if not self.llm:
            self.load_llm()
        if not self.retriever:
            raise ValueError("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¨¼ì € ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.")

        # ì²´ì¸ êµ¬ì„±: ê²€ìƒ‰ëœ context + question â†’ í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ ì¶œë ¥ íŒŒì‹±
        self.rag_chain = (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def chat(self, prompt: str) -> str:
        """
        ì‚¬ìš©ìì˜ ì…ë ¥ ë©”ì‹œì§€(ì§ˆë¬¸)ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ì‚¬ì „ ì²´ì¸ êµ¬ì„±ì´ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

        :param prompt: ì‚¬ìš©ì ì§ˆë¬¸
        :return: ìƒì„±ëœ ë‹µë³€
        """
        if not self.rag_chain:
            raise ValueError("RAG ì²´ì¸ì´ ì•„ì§ êµ¬ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self.rag_chain.invoke(prompt)
