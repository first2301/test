from langchain.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

class ChatbotService:
    def __init__(self, model):
        self.model = model

    def generate_response(self, prompt):
        # Here you would implement the logic to interact with your model
        # For example, using OpenAI's API or any other model you have
        response = self.model.generate(prompt)
        return response
    
    def get_prompt(self):
        system_prompt = """
                        ë‹¹ì‹ ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ë©°, ì •ë³´ ì „ë‹¬ê³¼ ë§¥ë½ ì´í•´ì— ì§‘ì¤‘í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.

                        â— ì£¼ì˜ì‚¬í•­:
                        - **ì ˆëŒ€ ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ AIê°€ ë¨¼ì € ì§ˆë¬¸í•˜ê±°ë‚˜ ëŒ€í™”ë¥¼ ì´ëŒì–´ê°€ì§€ ë§ˆì„¸ìš”.**
                        - **ì‚¬ìš©ìê°€ ë§í•˜ê¸° ì „ì—ëŠ” AIê°€ ìì˜ì ìœ¼ë¡œ ë°œí™”í•˜ê±°ë‚˜ ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” í–‰ë™ì„ ê¸ˆì§€í•©ë‹ˆë‹¤.**
                        - ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ì§€ ì•Šì•˜ë”ë¼ë„, AIê°€ ë¨¼ì € ì¶”ì¸¡í•˜ì—¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ê±°ë‚˜ ê·¸ì— ëŒ€í•´ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

                        ğŸ¤– ì—­í•  ë° ì‘ë‹µ ë°©ì‹:
                        - ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê³¼ê±° ëŒ€í™”(history)ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ë§¥ë½ì„ ì´í•´í•˜ê³ ** ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
                        - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆë¶„ëª…í•œ ê²½ìš°ì—ë§Œ **ì§§ê³  ëª…í™•í•œ í™•ì¸ ì§ˆë¬¸**ìœ¼ë¡œ ë³´ì™„ ì •ë³´ë¥¼ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        - ì„¤ëª…ì€ êµ¬ì²´ì ì´ê³  ë‹¨ê³„ì ìœ¼ë¡œ êµ¬ì„±í•˜ë©°, í•„ìš”í•  ê²½ìš° ë²ˆí˜¸(1., 2., 3.) ë˜ëŠ” ë¶ˆë¦¿í¬ì¸íŠ¸(â€¢) í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
                        - ë¶ˆí•„ìš”í•˜ê²Œ ê¸¸ê±°ë‚˜ ë°˜ë³µì ì¸ ì„¤ëª…ì€ í”¼í•˜ê³ , í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
                        - ëª¨ë“  ë‹µë³€ì€ **ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´**ë¡œ ì‘ì„±í•´ì•¼ í•˜ë©°, ì™¸êµ­ì–´(ì˜ì–´, í•œì ë“±)ëŠ” **ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³´ì¡°ë¡œ ê°„ë‹¨íˆ ì²¨ë¶€**í•˜ì„¸ìš”.
                        - ë³µì¡í•˜ê±°ë‚˜ ìƒì†Œí•œ ê°œë…ì€ ì¼ìƒì ì¸ ì˜ˆì‹œë‚˜ ì‰¬ìš´ ì–¸ì–´ë¡œ í’€ì–´ ì„¤ëª…í•˜ì„¸ìš”.
                        - ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
                        - ì™¸êµ­ì–´(ì˜ì–´, í•œì ë“±)ëŠ” ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ê´„í˜¸ ì† ì§§ì€ ë³´ì¡° ì„¤ëª…ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
                        - í•œê¸€ ì™¸ ë‹¨ì–´ê°€ ìë™ìœ¼ë¡œ ì‚½ì…ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ë©°, íŠ¹íˆ ëª…ì‚¬Â·í˜•ìš©ì‚¬ ë“±ì€ í•œêµ­ì–´ í‘œí˜„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
                        - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë¶ˆë¶„ëª…í•œ ê²½ìš°, ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ í†µí•´ ì§ˆë¬¸ì„ ëª…í™•íˆ í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.
                        - **ë™ì¼í•œ ë¬¸ì¥ì„ ë°˜ë³µí•˜ì§€ ë§ê³ **, ê° ë¬¸ë‹¨ë§ˆë‹¤ ìƒˆë¡­ê³  ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
                        - **ë™ì¼í•œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì„ ë°˜ë³µí•˜ì§€ ë§ê³ **, ì‘ë‹µì€ ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.
                        """
        template = f"""{system_prompt}
                    ### ëŒ€í™” ì´ë ¥:
                    {{history}}

                    ### ì‚¬ìš©ì: {{input}}
                    AI:"""
        self.prompt = PromptTemplate(
            input_variables=["history", "input"],
            template=template
        )



    def get_llm(self):
        """
        ìƒí™©	       / ì¶”ì²œ ì„¤ì •
        ê¸°ì—…ìš© ì±—ë´‡    / ì •í™•ì„± ìš°ì„ 	temperature=0.3 ~ 0.5
        ì •ë³´ ìš”ì•½     / ë²ˆì—­	temperature=0.0 ~ 0.3
        ì•„ì´ë””ì–´ ìƒì„± / ì°½ì˜ì  ì§ˆë¬¸	temperature=0.7 ~ 0.9
        ì‹¤í—˜ì  ì´ì•¼ê¸° ìƒì„±	temperature=1.0 ì´ìƒ
        """
        self.llm = LlamaCpp(
            # model_path="../ai_models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q8_0.gguf",
            model_path="../../ai_models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
            n_ctx=4096, # 2048 / 4096
            n_batch=64,
            n_threads=8,
            temperature=0.6, # 0.5 / 0.6 / 0.7
            max_tokens=1024,
            stop=["ì‚¬ìš©ì:", "User:"], # , "AI:", "Assistant:"
            verbose=True
        )

    def get_chain(self):
        self.memory = ConversationBufferMemory(return_messages=False) # memory_key="history", 
        self.chain = LLMChain(
            llm=self.llm,
            prompt=self.prompt,
            memory=self.memory,
            verbose=True
        )

    def chat(self, message: str) -> str:
        return self.chain.predict(input=message)